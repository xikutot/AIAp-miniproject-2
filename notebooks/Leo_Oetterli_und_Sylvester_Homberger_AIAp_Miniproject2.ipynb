{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# AIAp Miniproject 2\n",
    "\n",
    "## Contents\n",
    "\n",
    "- [Imports](#imports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras import utils as keras_utils\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T14:17:35.100344Z",
     "start_time": "2024-04-02T14:17:34.992972Z"
    }
   },
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "SEED = 42"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T14:09:41.871335Z",
     "start_time": "2024-04-02T14:09:41.869102Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### b) Citation and description"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For our project we used the following data:\n",
    "\n",
    "- **Title:** Architectural Heritage Elements Dataset\n",
    "- **Subtitle:** 128 (creative commons) revised\n",
    "- **Version:** 1\n",
    "- **Publication Date:** 2017-02-20\n",
    "- **Author:** Jose Llamas\n",
    "- **Organization:** Fundaci√≥n CARTIF\n",
    "- **Source:** https://correo.cartif.es/home/joslla@cartif.es/Briefcase/Architectural_Heritage_Elements_image_Dataset/Architectural_Heritage_Elements_Dataset_128%28creative_commons%29.zip\n",
    "- **Media:** .jpg in .zip\n",
    "- **Download:** 2024-03-26\n",
    "\n",
    "The raw data was extracted with Windows. We then added it to our [\"AIAp Miniproject 2\"](https://gitlab.ost.ch/sylvester.homberger/aiap-miniproject-2) repository on the Gitlab instance of OST.\n",
    "\n",
    "This dataset consists of 10437 RGB 64x64 jpg images classified in 11 categories:\n",
    "- Altar: 828 images\n",
    "- Apse: 505 images\n",
    "- Bell tower: 1057 images\n",
    "- Column: 1914 images\n",
    "- Dome (inner): 589 images\n",
    "- Dome (outer): 1175 images\n",
    "- Flying buttress: 405 images\n",
    "- Gargoyle (and Chimera): 1562 images\n",
    "- Portal: 307 images\n",
    "- Stained glass: 998 images\n",
    "- Vault: 1097 images\n",
    "\n",
    "As \"flying buttress\" and \"portal\" do not satisfy the minimal samples requirement of 500, we manually moved them out of our dataset into a separate folder. Our adjusted dataset therefore fulfils the requirements:\n",
    "- RGB images of 9 classes\n",
    "- Each has more than 500 samples and the total is 9725 images\n",
    "- They have a resolution of 64x64 pixels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_folder = Path(os.getcwd()).parent / \"data\"\n",
    "raw_folder = data_folder / \"raw\" / \"Architectural_Heritage_Elements_Dataset_64(creative_commons)_revised\"\n",
    "\n",
    "training_dataset = keras_utils.image_dataset_from_directory(\n",
    "  raw_folder,\n",
    "  image_size=(64, 64),\n",
    "  seed=SEED,\n",
    "  validation_split=0.3,\n",
    "  subset=\"training\"\n",
    ")\n",
    "\n",
    "test_dataset = keras_utils.image_dataset_from_directory(\n",
    "  raw_folder,\n",
    "  image_size=(64, 64),\n",
    "  seed=SEED,\n",
    "  validation_split=0.3,\n",
    "  subset=\"validation\"\n",
    ")\n",
    "\n",
    "label_to_class = training_dataset.class_names\n",
    "class_to_label = dict(zip(label_to_class, range(len(label_to_class))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### c) Splitting into training and test data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "train_data, validation_data = keras_utils.split_dataset(training_dataset, left_size=0.8, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exploratory data analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Samples per class\n",
    "\n",
    "class_counts = train_val_data.reduce(\n",
    "    initial_state=tf.zeros(len(label_to_class), dtype=tf.int32),\n",
    "    reduce_func=lambda count, images_labels: count + tf.math.bincount(images_labels[1], minlength=len(dataset.class_names)))\n",
    "\n",
    "class_counts_values = [count.numpy() for count in class_counts]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "bars = plt.bar(label_to_class, class_counts_values, color='skyblue')\n",
    "plt.bar_label(bars, labels=class_counts_values, label_type='edge', color='black')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Sample Count')\n",
    "plt.title('Sample Count per Class')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The dataset is not balanced. Classes need to be weighted for training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Plot a few images\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, labels in train_val_data.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(label_to_class[labels[i]])\n",
    "    plt.axis(\"off\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Architecture 1 (underfitting)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Architecture 2 (overfitting)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Architecture 2 (optimized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Quantification of the model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiap-miniproject-2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
